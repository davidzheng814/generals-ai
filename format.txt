Engine Output Interface
-----------------------

Sets settings for the game:
setting num_players [num_players]
setting start_player [start_player]
setting board_width [board_width]
setting board_height [board_height]
setting usernames [username0] [username1] ...

Piece Format:
type=[type][,owner=[player]][,size=[size]]

Map Layout:
action new_piece [new_piece] [loc]

Move:
1. action next_move [player] (Called at the beginning of each new move.)
2. action [move [new_piece1] [new_piece2] [loc1] [loc2]]|[no_move]
3. action set_piece [new_piece] [loc]
4. action set_land [player] [size]
5. action set_army [player] [size]

Engine Input Interface
----------------------

setting board_width [board_width]
setting board_height [board_height]
setting start_player [player]
setting usernames [username0] [username1] ...
setting generals [loc0] [loc1] ...
setting mountains [loc0] [loc1] ...
setting cities [loc0] [loc1] ...
start
action move [loc1] [loc2]
action half_move [loc1] [loc2]
action no_move

Model 1p training
-----------------
input: [num_players] [board_width] [board_height]
  optional input: "quit"
  input: state as single-line json: {'board':[[]], 'valid_moves':[3, 8, 20]}
  output: action as [move]
  input: [scalar reward]

Model Evaluation:
-----------------
Let Q = online Q network
Let Q- = target Q network
a = argmax_a Q(s, a)

on update:
  - sample batch with (r_j, s_{j+1})
  - calc rewards = r_j + max_a Q-(s_{j+1,}, a)
  - Perform update on (y_j - Q(

Game State Tensor:
------------------
0-1: General (1 at square)
2-3: Army of One Unit (1 at squares)
4-5: Army of >= 2 Units (#army at squares)
6: Mountains (1 at squares)
7: Neutral Cities (#army at squares)
8-9: Cities (#army at squares)
